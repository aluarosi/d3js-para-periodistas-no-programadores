{
  "version": "0.0.6",
  "name": "csv-parse",
  "description": "CSV parsing implementing the Node.js `stream.Transform` API",
  "keywords": [
    "csv",
    "parse",
    "parser"
  ],
  "contributors": [
    {
      "name": "David Worms",
      "email": "david@adaltas.com",
      "url": "http://www.adaltas.com"
    },
    {
      "name": "Will White",
      "url": "https://github.com/willwhite"
    },
    {
      "name": "Justin Latimer",
      "url": "https://github.com/justinlatimer"
    },
    {
      "name": "jonseymour",
      "url": "https://github.com/jonseymour"
    },
    {
      "name": "pascalopitz",
      "url": "https://github.com/pascalopitz"
    },
    {
      "name": "Josh Pschorr",
      "url": "https://github.com/jpschorr"
    },
    {
      "name": "Elad Ben-Israel",
      "url": "https://github.com/eladb"
    },
    {
      "name": "Philippe Plantier",
      "url": "https://github.com/phipla"
    },
    {
      "name": "Tim Oxley",
      "url": "https://github.com/timoxley"
    },
    {
      "name": "Damon Oehlman",
      "url": "https://github.com/DamonOehlman"
    },
    {
      "name": "Alexandru Topliceanu",
      "url": "https://github.com/topliceanu"
    },
    {
      "name": "Visup",
      "url": "https://github.com/visup"
    },
    {
      "name": "Edmund von der Burg",
      "url": "https://github.com/evdb"
    },
    {
      "name": "Douglas Christopher Wilson",
      "url": "https://github.com/dougwilson"
    },
    {
      "name": "Joe Eaves",
      "url": "https://github.com/Joeasaurus"
    },
    {
      "name": "Mark Stosberg",
      "url": "https://github.com/markstos"
    }
  ],
  "licenses": [
    {
      "type": "BSD",
      "url": "https://github.com/wdavidw/node-csv-parse/blob/master/LICENSE"
    }
  ],
  "repository": {
    "type": "git",
    "url": "http://www.github.com/wdavidw/node-csv-parse"
  },
  "dependencies": {},
  "devDependencies": {
    "each": "latest",
    "coffee-script": "latest",
    "csv-generate": "latest",
    "csv-spectrum": "latest",
    "mocha": "latest",
    "should": "latest"
  },
  "optionalDependencies": {},
  "main": "./lib",
  "scripts": {
    "test": "make test"
  },
  "readme": "[![Build Status](https://secure.travis-ci.org/wdavidw/node-csv-parse.png)](http://travis-ci.org/wdavidw/node-csv-parse)\n\nPart of the [CSV module](https://github.com/wdavidw/node-csv), this project is a\nparser converting CSV text input into arrays or objects. It implements the \nNode.js [stream.Transform`API](http://nodejs.org/api/stream.html#stream_class_stream_transform). It also provides a simple callback-base API for convenience. It is both extremely easy to use and powerful. It was first\nreleased in 2010 and is used against big data sets by a large community.\n\n[The full documentation of the CSV parser is available here](http://www.adaltas.com/projects/node-csv/).\n\n## Features\n\n*   Follow the Node.js streaming API\n*   Support delimiters, quotes, escape characters and comments\n*   Line breaks discovery\n*   Support big datasets\n*   Complete test coverage and samples for inspiration\n*   no external dependencies\n*   to be used conjointly with `csv-generate`, `stream-transform` and `csv-stringify`\n\n\n## Usage\n\nRun `npm install csv` to install the full CSV package or run \n`npm install csv-parse` if you are only interested by the CSV parser.\n\nUse the callback style API for simplicity or the stream based API for \nscalability. You may also mix the two styles. For example, the \n[fs_read.js example][fs_read] pipe a file stream reader and get the results \ninside a callback.\n\nFor examples, refer to [the \"samples\" folder][csv-samples], \nthe documentation or [the \"test\" folder][csv-test].\n\n### Using the callback API\n\nThe parser receive a string and returns an array inside a user-provided \ncallback. This example is available with the command `node samples/callback.js`.\n\nSee the full list of supported parsing options below.\n\n```javascript\nvar parse = require('csv-parse');\nrequire('should');\n\nvar input = '#Welcome\\n\"1\",\"2\",\"3\",\"4\"\\n\"a\",\"b\",\"c\",\"d\"';\nparse(input, {comment: '#'}, function(err, output){\n  output.should.eql([ [ '1', '2', '3', '4' ], [ 'a', 'b', 'c', 'd' ] ]);\n});\n```\n\n### Using the stream API\n\nThe CSV parser implements the [stream.Transform`API][stream_transform].\n\nCSV data is send through the `write` function and the resulted data is obtained\nwithin the \"readable\" event by calling the `read` function. This example is \navailable with the command `node samples/stream.js`.\n\nSee the full list of supported parser options below.\n    \n```javascript\nvar parse = require('csv-parse');\nrequire('should');\n\nvar output = [];\n// Create the parser\nvar parser = parse({delimiter: ':'});\n// Use the writable stream api\nparser.on('readable', function(){\n  while(record = parser.read()){\n    output.push(record);\n  }\n});\n// Catch any error\nparser.on('error', function(err){\n  console.log(err.message);\n});\n// When we are done, test that the parsed output matched what expected\nparser.on('finish', function(){\n  output.should.eql([\n    [ 'root','x','0','0','root','/root','/bin/bash' ],\n    [ 'someone','x','1022','1022','a funny cat','/home/someone','/bin/bash' ]\n  ]);\n});\n// Now that setup is done, write data to the stream\nparser.write(\"root:x:0:0:root:/root:/bin/bash\\n\");\nparser.write(\"someone:x:1022:1022:a funny cat:/home/someone:/bin/bash\\n\");\n// Close the readable stream\nparser.end();\n```\n\n### Using the pipe function\n\nOne useful function part of the Stream API is `pipe` to interact between \nmultiple streams. You may use this function to pipe a `stream.Readable` string \nsource to a `stream.Writable` object destination. This example available as \n`node samples/pipe.js` read the file, parse its content and transform it.\n\n```javascript\nvar fs = require('fs');\nvar parse = require('csv-parse');\nvar transform = require('stream-transform');\n\nvar output = [];\nvar parser = parse({delimiter: ':'})\nvar input = fs.createReadStream('/etc/passwd');\nvar transformer = transform(function(record, callback){\n  setTimeout(function(){\n    callback(null, record.join(' ')+'\\n');\n  }, 500);\n}, {parallel: 10});\ninput.pipe(parser).pipe(transformer).pipe(process.stdout);\n```\n\n## Parser options\n\n*   `delimiter`         Set the field delimiter. One character only, defaults to comma.   \n*   `rowDelimiter`      String used to delimit record rows or a special value; special values are 'auto', 'unix', 'mac', 'windows', 'unicode'; defaults to 'auto' (discovered in source or 'unix' if no source is specified).   \n*   `quote`             Optional character surrounding a field, one character only, defaults to double quotes.   \n*   `escape`            Set the escape character, one character only, defaults to double quotes.   \n*   `columns`           List of fields as an array, a user defined callback accepting the first line and returning the column names or true if autodiscovered in the first CSV line, default to null, affect the result data set in the sense that records will be objects instead of arrays.   \n*   `comment`           Treat all the characteres after this one as a comment, default to '#'.   \n*   `objname`           Name of header-record title to name objects by.   \n*   `skip_empty_lines`  Dont generate empty values for empty lines.   \n*   `trim`              If true, ignore whitespace immediately around the delimiter, defaults to false.   \n*   `ltrim`             If true, ignore whitespace immediately following the delimiter (i.e. left-trim all fields), defaults to false.   \n*   `rtrim`             If true, ignore whitespace immediately preceding the delimiter (i.e. right-trim all fields), defaults to false.   \n*   `auto_parse`        If true, the parser will attempt to convert read data types to native types.   \n\n## Migration\n\nMost of the generator is imported from its parent project [CSV][csv] in a effort \nto split it between the generator, the parser, the transformer and the \nstringifier.\n\nThe \"record\" has disappeared, you are encouraged to use the \"readable\" event conjointly \nwith the \"read\" function as documented above and in the [Stream API][stream_transform].\n\n## Development\n\nTests are executed with mocha. To install it, simple run `npm install` \nfollowed by `npm test`. It will install mocha and its dependencies in your \nproject \"node_modules\" directory and run the test suite. The tests run \nagainst the CoffeeScript source files.\n\nTo generate the JavaScript files, run `make build`.\n\nThe test suite is run online with [Travis][travis] against the versions \n0.10 and 0.11 of Node.js.\n\nContributors\n------------\n\n*   David Worms: <https://github.com/wdavidw>\n*   Will White: <https://github.com/willwhite>\n*   Justin Latimer: <https://github.com/justinlatimer>\n*   jonseymour: <https://github.com/jonseymour>\n*   pascalopitz: <https://github.com/pascalopitz>\n*   Josh Pschorr: <https://github.com/jpschorr>\n*   Elad Ben-Israel: <https://github.com/eladb>\n*   Philippe Plantier: <https://github.com/phipla>\n*   Tim Oxley: <https://github.com/timoxley>\n*   Damon Oehlman: <https://github.com/DamonOehlman>\n*   Alexandru Topliceanu: <https://github.com/topliceanu>\n*   Visup: <https://github.com/visup>\n*   Edmund von der Burg: <https://github.com/evdb>\n*   Douglas Christopher Wilson: <https://github.com/dougwilson>\n*   Chris Khoo: <https://github.com/khoomeister>\n*   Joeasaurus: <https://github.com/Joeasaurus>\n*   Mark Stosberg: <https://github.com/markstos>\n\n[csv]: https://github.com/wdavidw/node-csv\n[csv-samples]: https://github.com/wdavidw/node-csv-parse/tree/master/samples\n[fs_read]: https://github.com/wdavidw/node-csv-parse/tree/master/samples/fs_read.js\n[csv-test]: https://github.com/wdavidw/node-csv-parse/tree/master/test\n[stream_transform]: http://nodejs.org/api/stream.html#stream_class_stream_transform\n[travis]: https://travis-ci.org/#!/wdavidw/node-csv-parse\n\n",
  "readmeFilename": "README.md",
  "_id": "csv-parse@0.0.6",
  "_shasum": "94610722650feac81cf549c2c9298632d2b6037c",
  "_from": "csv-parse@*",
  "_resolved": "https://registry.npmjs.org/csv-parse/-/csv-parse-0.0.6.tgz"
}
